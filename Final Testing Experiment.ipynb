{"cells":[{"cell_type":"markdown","metadata":{"id":"NXkwrTL7dAfm"},"source":["# **Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10976,"status":"ok","timestamp":1654578076458,"user":{"displayName":"Zoha A","userId":"02342256513199911633"},"user_tz":240},"id":"RAX3bPab_OWk","outputId":"3fbb20c7-b262-4eb3-81e6-a72667e03e01"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/sadl11\n","/content/drive/MyDrive/sadl11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.0)\n","Collecting absl-py==0.2.2\n","  Using cached absl-py-0.2.2.tar.gz (82 kB)\n","Collecting setuptools==39.1.0\n","  Using cached setuptools-39.1.0-py2.py3-none-any.whl (566 kB)\n","Collecting astor==0.7.1\n","  Using cached astor-0.7.1-py2.py3-none-any.whl (27 kB)\n","Collecting gast==0.2.0\n","  Using cached gast-0.2.0.tar.gz (9.4 kB)\n","Collecting grpcio==1.13.0\n","  Using cached grpcio-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (9.1 MB)\n","Collecting h5py==2.8.0\n","  Using cached h5py-2.8.0-cp37-cp37m-manylinux1_x86_64.whl (2.8 MB)\n","Collecting Keras==2.2.0\n","  Using cached Keras-2.2.0-py2.py3-none-any.whl (300 kB)\n","Collecting Keras-Applications==1.0.2\n","  Using cached Keras_Applications-1.0.2-py2.py3-none-any.whl (43 kB)\n","Collecting Keras-Preprocessing==1.0.1\n","  Using cached Keras_Preprocessing-1.0.1-py2.py3-none-any.whl (26 kB)\n","Collecting Markdown==2.6.11\n","  Using cached Markdown-2.6.11-py2.py3-none-any.whl (78 kB)\n","Collecting numpy==1.14.5\n","  Using cached numpy-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (12.2 MB)\n","Collecting protobuf==3.6.0\n","  Using cached protobuf-3.6.0-py2.py3-none-any.whl (390 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.13)\n","Collecting scipy==1.1.0\n","  Using cached scipy-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (31.2 MB)\n","Collecting six==1.11.0\n","  Using cached six-1.11.0-py2.py3-none-any.whl (10 kB)\n","Collecting tensorboard==1.9.0\n","  Using cached tensorboard-1.9.0-py3-none-any.whl (3.3 MB)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.9.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.0+zzzcolab20220506153740, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.4+zzzcolab20220516125453, 2.6.5, 2.6.5+zzzcolab20220523104206, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.0+zzzcolab20220506150900, 2.7.1, 2.7.2, 2.7.2+zzzcolab20220516114640, 2.7.3, 2.7.3+zzzcolab20220523111007, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.0+zzzcolab20220506162203, 2.8.1, 2.8.1+zzzcolab20220516111314, 2.8.1+zzzcolab20220518083849, 2.8.2, 2.8.2+zzzcolab20220523105045, 2.8.2+zzzcolab20220527125636, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tensorflow==1.9.0\u001b[0m\n"]}],"source":["from numpy import argmax\n","import random\n","import seaborn as sbn\n","from numpy.random import rand, randn\n","from scipy.linalg import qr\n","from numpy import ones\n","from scipy import stats\n","import scipy\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from numpy import linalg as LA\n","import  array\n","import math\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from sklearn import linear_model\n","import sklearn\n","from tabulate import tabulate\n","from math import sqrt\n","# from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from sklearn.decomposition import PCA as sklearnPCA\n","import copy\n","import time\n","from keras import backend as K\n","import argparse\n","import shutil\n","import warnings\n","import keras.backend as KeyboardInterrupt\n","from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n","from keras.regularizers import l2\n","from keras.models import load_model, Model\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from scipy.io import loadmat\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.metrics import confusion_matrix\n","from keras import layers\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import VGG16\n","import tensorflow as tf\n","from keras.datasets import mnist, cifar10 , fashion_mnist, cifar100\n","from keras.preprocessing.image import img_to_array, array_to_img\n","import sys\n","sys.path.append('..')\n","\n","\n","!pwd\n","import os\n","os.chdir('/content/drive/MyDrive/sadl11')\n","!pwd\n","#MinMAx\n","def scale(X, x_min, x_max):\n","    nom = (X-X.min(axis=0))*(x_max-x_min)\n","    denom = X.max(axis=0) - X.min(axis=0)\n","    denom[denom==0] = 1\n","    return x_min + nom/denom \n","\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"_xDfeOp6Y3hL"},"source":["# **Dataset and Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2Jj8YHEv01I"},"outputs":[],"source":["############################################## Dataset $ model  ########################################################################\n","#############################################Function description#######################################################################\n","\n","#________________________________________________(dataset)___________________________________________________________\n","#It takes the name of the dataset , and corresponding model from the user then it preprocess the dataset and load the model.\n","#______________Input: arg (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\")\n","#______________Output: 1- Training and Testing set of the given dataset\n","#______________________2- the noramlized version of feature matrix of the given dataset (VGG features)\n","#______________________3- load the trained model on the given dataset\n","#______________________4- path of the folder of stored random inputs and thier results\n","\n","def dataset(arg, model_name):\n","  CLIP_MIN = -0.5\n","  CLIP_MAX = 0.5\n","\n","  if arg==\"mnist\":\n","    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","    x_train = x_train.reshape(-1, 28, 28, 1)\n","    x_test = x_test.reshape(-1, 28, 28, 1)\n","    x_train = x_train.astype(\"float32\")\n","    x_test = x_test.astype(\"float32\")\n","    # ##Model\n","    if model_name==\"LeNet1\":\n","        model=load_model(\"/content/drive/MyDrive/CE/sadl11/model/model_mnist_LeNet1.h5\")\n","        path=\"/content/drive/MyDrive/CE/RQ2_3/Correlation/BB_mnist_BB_LeNet1/Size\"\n","    if model_name==\"LeNet5\":\n","        model=load_model(\"/content/drive/MyDrive/CE/sadl11/model/model_mnist_LeNet5.h5\")\n","        path=\"/content/drive/MyDrive/CE/RQ2_3/Correlation/BB_mnist_BB_LeNet5/Size\"\n","    ##VGG feature extaction (4068)\n","    ##rank of features (2476)\n","    #You can use the stored verion or call vgg function to extract features(comment out the second line)\n","    MNIST_VGG=np.load(\"/content/drive/MyDrive/Extracted Features/MNIST/block5_conv3_3_3_512.npy\")\n","    #_,MNIST_VGG=vgg16_features_GD(\"mnist\")\n","    features_vgg=MNIST_VGG\n","    y_test = np_utils.to_categorical(y_test, 10)\n","    y_test=np.argmax(y_test, axis=1)  \n","    y_train = np_utils.to_categorical(y_train, 10)\n","\n","  if arg==\"Fashion_mnist\":\n","    # load dataset\n","    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","    x_train = x_train.reshape(-1, 28, 28, 1)\n","    x_test = x_test.reshape(-1, 28, 28, 1)\n","    x_train = x_train.astype(\"float32\")\n","    x_test = x_test.astype(\"float32\")\n","    _,features_vgg=vgg16_features_GD(\"Fashion_mnist\")\n","    if model_name==\"LeNet4\":\n","      model=load_model(\"/content/drive/MyDrive/sadl11/model/model_fashion_mnist_LeNet4.h5\")\n","      path=\"/content/drive/MyDrive/RQ2_3/Correlation/BB_Fashion_mnist_LeNet4/\"\n","      y_test = np_utils.to_categorical(y_test, 10)\n","      y_test=np.argmax(y_test, axis=1)  \n","      y_train = np_utils.to_categorical(y_train, 10)\n","\n","  \n","  if arg==\"SVHN\":\n","    train_raw = loadmat('/content/drive/MyDrive/Data/train_32x32.mat')\n","    test_raw = loadmat('/content/drive/MyDrive/Data/test_32x32.mat')\n","    x_train = np.array(train_raw['X'])\n","    x_test = np.array(test_raw['X'])\n","    y_train = train_raw['y']\n","    y_test = test_raw['y']\n","    x_train = np.moveaxis(x_train, -1, 0)\n","    x_test = np.moveaxis(x_test, -1, 0)\n","    x_test= x_test.reshape (-1,32,32,3)\n","    x_train= x_train.reshape (-1,32,32,3)\n","    x_train = x_train.astype(\"float32\")\n","    x_test = x_test.astype(\"float32\")\n","    _,features_vgg=vgg16_features_GD(\"SVHN\")\n","    if model_name==\"LeNet5\":\n","      model=load_model(\"/content/drive/MyDrive/sadl11/model/model_SVHN_LeNet5.h5\")\n","      path=\"/content/drive/MyDrive/RQ2_3/Correlation/BB_SVHN_LeNet5/\"\n","      lb = LabelBinarizer()\n","      y_train = lb.fit_transform(y_train)\n","      y_test = lb.fit_transform(y_test)\n","      y_test=np.argmax(y_test, axis=1)\n","\n","  if arg==\"cifar10\":\n","    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    x_train = x_train.astype(\"float32\")\n","    x_test = x_test.astype(\"float32\")\n","    if model_name==\"12Conv\":\n","      model = load_model('/content/drive/MyDrive/sadl11/model/model_cifar.h5')\n","      path=\"/content/drive/MyDrive/RQ2_3/Correlation/BB_cifar_BB_vgg_12conv/\"\n","    if model_name==\"ResNet20\":\n","      model = load_model('/content/drive/MyDrive/sadl11/model/Cifar10_resnet20.h5')\n","      path=\"/content/drive/MyDrive/RQ2_3/Correlation/BB_cifar10_ResNet20/\"\n","  \n","    #VGG feature extaction (4068)\n","    #rank of features (3845)\n","    #You can use the stored verion or call vgg function to extract features (comment out the second line)\n","    Cifar_VGG=np.load(\"/content/drive/MyDrive/Extracted Features/Cifar10/x_cifar_inputshape48_block5_conv3.npy\")\n","    #_,Cifar_VGG=vgg16_features_GD(\"cifar10\")\n","    features_vgg=Cifar_VGG\n","    y_test = np_utils.to_categorical(y_test, 10)\n","    y_test=np.argmax(y_test, axis=1)  \n","    y_train = np_utils.to_categorical(y_train, 10)\n","  \n","  x_train = (x_train / 255.0) - (1.0 - CLIP_MAX)\n","  x_test = (x_test / 255.0) - (1.0 - CLIP_MAX)\n","\n","  return x_train, y_train, x_test, y_test,features_vgg , model, path\n"]},{"cell_type":"markdown","metadata":{"id":"VqNuJSU8YvYx"},"source":["# **Feature Extraction**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7Kzt-nz-pYv"},"outputs":[],"source":["#________________________________________________(vgg16_features_GD)___________________________________________________________\n","#It needs the name of the dataset , then it extract the features of this datasets by using VGG16 (the internal layer: 'block5_conv3') \n","#to have enough number of features for calculating the GD (geometric diversity score)\n","\n","#______________Input: arg (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\")\n","#______________Output: 1- Original Feature Matrix of test inputs of the given dataset (features)\n","#______________________2- the noramlized version of feature matrix between 0 and 1 (X_scf)\n","\n","\n","def vgg16_features_GD(arg):\n","\n","  CLIP_MIN = -0.5\n","  CLIP_MAX = 0.5\n","    # lb = LabelBinarizer()\n","    # train_labels = lb.fit_transform(train_labels)\n","    # test_labels = lb.fit_transform(test_labels)\n","  if (arg==\"cifar10\" or arg==\"cifar100\" or arg==\"SVHN\"):\n","    if(arg==\"cifar10\"):\n","      (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","    if(arg==\"cifar100\"):\n","      (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n","    if (arg==\"SVHN\"):\n","      train_raw = loadmat('/content/drive/MyDrive/Data/train_32x32.mat')\n","      test_raw = loadmat('/content/drive/MyDrive/Data/test_32x32.mat')\n","      x_train = np.array(train_raw['X'])\n","      x_test = np.array(test_raw['X'])\n","      y_train = train_raw['y']\n","      y_test = test_raw['y']\n","      x_train = np.moveaxis(x_train, -1, 0)\n","      x_test = np.moveaxis(x_test, -1, 0)\n","      # lb = LabelBinarizer()\n","      # train_labels = lb.fit_transform(train_labels)\n","      # test_labels = lb.fit_transform(test_labels)\n","\n","    x_test1= x_test.reshape (-1,32,32,3)\n","\n","  if (arg ==\"mnist\" or arg==\"Fashion_mnist\"):\n","    if (arg==\"mnist\"):\n","      (x_train, y_train), (x_test, y_test) = mnist.load_data()\n","    if(arg==\"Fashion_mnist\"):\n","      (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","    x_test1=np.dstack([x_test]*3)\n","    x_test1= x_test1.reshape(-1,28,28,3)\n","    #Resize the images 48*48 as required by VGG16\n","\n","  x_test1 = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in x_test1])\n","  \n","  x_test1 = x_test1.astype(\"float32\")\n","  x_test1 = (x_test1 / 255.0) - (1.0 - CLIP_MAX)\n","  input_layer=layers.Input(shape=(48,48,3))\n","  model_vgg16=VGG16(weights='imagenet',input_tensor=input_layer,include_top=False)\n","  model_vgg16.summary()\n","  base_model = model_vgg16\n","  # You can select another layer of VGG16 that you want to test.\n","  name_layer = 'block5_conv3'\n","  intermediate_layer_model = Model(inputs=base_model.input, outputs=base_model.get_layer(name_layer).output)\n","  FF = intermediate_layer_model.predict(x_test1)\n","  features= FF.reshape((len(x_test1),9*512))\n","  # print(\"rank of feature matrix\", np.linalg.matrix_rank(features))\n","  nom = (features-features.min(axis=0))*(1-0)\n","  denom = features.max(axis=0) - features.min(axis=0)\n","  denom[denom==0] = 1\n","  X_scf = nom/denom \n","  print(X_scf)\n","  print(\"rank of feature matrix\", np.linalg.matrix_rank(X_scf))\n","  \n","  return features, X_scf"]},{"cell_type":"markdown","metadata":{"id":"6RQQRifAMGVM"},"source":["# **Fault definition**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgwnUjDAkMxL"},"outputs":[],"source":["\n","#________________________________________________(FaultFunction)___________________________________________________________\n","#It takes the name of the dataset , and corresponding model name then it loads the HDBSCAN clustering results of mispredicted inputs (from fault definition file).\n","#and it returns all necessary inforamtions about clustering, best clustering results after hyperparameter tuneing (index_clustering) and mispredicted inputs\n","\n","#______________Input: dataset (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\") model_name(\"LeNet1\", \"LeNet5\",\"LeNet4\",\"12Conv\",\"ResNet20\" ), dataset (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\")\n","#______________Output: 1- Training and Testing set of the given dataset\n","#______________________2- the noramlized version of feature matrix of the given dataset (VGG features)\n","#______________________3- load the trained model on the given dataset\n","#______________________4- path of the folder of stored random inputs and thier results\n","\n","\n","\n","\n","def FaultFunction(data_name,model_name):\n","  i=0\n","  if model_name==\"LeNet1\" and data_name==\"mnist\":\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet1_BBMNIST/top_clustering_label_2coulmns2.npy\")\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet1_BBMNIST/mis_index_testdataset_MNIST_LeNet1.npy\") \n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet1_BBMNIST/mis_index_traindataset_MNIST_LeNet1.npy\")\n","    index_clustering_config=2\n","  if model_name==\"LeNet5\" and data_name==\"mnist\":\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_BBMNIST/mis_index_testdataset_MNIST.npy\",allow_pickle=True)\n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_BBMNIST/mis_index_traindataset_MNIST.npy\")\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_BBMNIST/top_clustering_label_2coulmns.npy\")\n","    index_clustering_config=4\n","\n","  if model_name==\"12Conv\" and data_name==\"cifar10\":\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/Cifar_12conv_BB/Three_clustering_labels3(2coulmns).npy\")\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/Cifar_12conv_BB/mis_index_testdatasetcifar.npy\")\n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/Cifar_12conv_BB/mis_index_traindatasetcifar.npy\")\n","    index_clustering_config=0\n","  if model_name==\"ResNet20\" and data_name==\"cifar10\":\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/ResNet20_Cifar10/Three_clustering_labels.npy\")\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/ResNet20_Cifar10/mis_index_testdataset_cifRes.npy\")\n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/ResNet20_Cifar10/mis_index_traindataset_cifRes.npy\")\n","    index_clustering_config=4\n","\n","  if model_name==\"LeNet4\" and data_name==\"Fashion_mnist\":\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet4_fashion_MNIST/Three_clustering_labels.npy\")\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet4_fashion_MNIST/mis_index_testdataset_FMNIST.npy\")\n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet4_fashion_MNIST/mis_index_traindataset_FMNIST.npy\")\n","    index_clustering_config=13\n","  if model_name==\"LeNet5\" and data_name==\"SVHN\":\n","    clustering_result=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_SVHN/Three_clustering_labels.npy\")\n","    mis_ind_test=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_SVHN/mis_index_testdataset_SVHN.npy\")\n","    mis_ind_train=np.load(\"/content/drive/MyDrive/RQ2_3/Clustering/LeNet5_SVHN/mis_index_traindataset_SVHN.npy\")\n","    index_clustering_config=15\n","\n","  noisy_index=[]\n","  for i in range(len(mis_ind_test)):\n","    if clustering_result[index_clustering_config][i]==-1:\n","      noisy_index.append(mis_ind_test[i])\n","  sett=list(range(0, len(x_test)))\n","  index_withoutnoisy=set(sett)-set(noisy_index)\n","  id_test=index_withoutnoisy\n","\n","  return clustering_result[index_clustering_config],noisy_index, index_withoutnoisy, mis_ind_test, mis_ind_train"]},{"cell_type":"markdown","metadata":{"id":"p5iyrOidY-wA"},"source":["# **Main (Call Functions)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10289,"status":"ok","timestamp":1654578094839,"user":{"displayName":"Zoha A","userId":"02342256513199911633"},"user_tz":240},"id":"okBGmfh3v7Vj","outputId":"489ea2ef-b5ff-44f7-ed6e-2c13c56d00b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the dataset             mnist       /        cifar10       / Fashion_mnist /  SVHN  : cifar10\n","Enter the model name    LeNet1 or LeNet5/  12Conv or ResNet20  /     LeNet4    /  LeNet5 : ResNet20\n","cifar10 ResNet20\n","Model: \"res20-none\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," conv2d_42 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n","                                                                                                  \n"," batch_normalization_38 (BatchN  (None, 32, 32, 16)  64          ['conv2d_42[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_40 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_38[0][0]'] \n","                                                                                                  \n"," conv2d_43 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_40[0][0]']          \n","                                                                                                  \n"," batch_normalization_39 (BatchN  (None, 32, 32, 16)  64          ['conv2d_43[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_41 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_39[0][0]'] \n","                                                                                                  \n"," conv2d_44 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_41[0][0]']          \n","                                                                                                  \n"," dropout_18 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_44[0][0]']              \n","                                                                                                  \n"," add_18 (Add)                   (None, 32, 32, 16)   0           ['conv2d_42[0][0]',              \n","                                                                  'dropout_18[0][0]']             \n","                                                                                                  \n"," batch_normalization_40 (BatchN  (None, 32, 32, 16)  64          ['add_18[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_42 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_40[0][0]'] \n","                                                                                                  \n"," conv2d_45 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_42[0][0]']          \n","                                                                                                  \n"," batch_normalization_41 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_43 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_41[0][0]'] \n","                                                                                                  \n"," conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_43[0][0]']          \n","                                                                                                  \n"," dropout_19 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_46[0][0]']              \n","                                                                                                  \n"," add_19 (Add)                   (None, 32, 32, 16)   0           ['add_18[0][0]',                 \n","                                                                  'dropout_19[0][0]']             \n","                                                                                                  \n"," batch_normalization_42 (BatchN  (None, 32, 32, 16)  64          ['add_19[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_44 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_42[0][0]'] \n","                                                                                                  \n"," conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_44[0][0]']          \n","                                                                                                  \n"," batch_normalization_43 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_45 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_43[0][0]'] \n","                                                                                                  \n"," conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_45[0][0]']          \n","                                                                                                  \n"," dropout_20 (Dropout)           (None, 32, 32, 16)   0           ['conv2d_48[0][0]']              \n","                                                                                                  \n"," add_20 (Add)                   (None, 32, 32, 16)   0           ['add_19[0][0]',                 \n","                                                                  'dropout_20[0][0]']             \n","                                                                                                  \n"," batch_normalization_44 (BatchN  (None, 32, 32, 16)  64          ['add_20[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_46 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_44[0][0]'] \n","                                                                                                  \n"," conv2d_49 (Conv2D)             (None, 16, 16, 32)   4640        ['activation_46[0][0]']          \n","                                                                                                  \n"," batch_normalization_45 (BatchN  (None, 16, 16, 32)  128         ['conv2d_49[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_47 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_45[0][0]'] \n","                                                                                                  \n"," conv2d_50 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_47[0][0]']          \n","                                                                                                  \n"," dropout_21 (Dropout)           (None, 16, 16, 32)   0           ['conv2d_50[0][0]']              \n","                                                                                                  \n"," conv2d_51 (Conv2D)             (None, 16, 16, 32)   544         ['add_20[0][0]']                 \n","                                                                                                  \n"," add_21 (Add)                   (None, 16, 16, 32)   0           ['dropout_21[0][0]',             \n","                                                                  'conv2d_51[0][0]']              \n","                                                                                                  \n"," batch_normalization_46 (BatchN  (None, 16, 16, 32)  128         ['add_21[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_48 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_46[0][0]'] \n","                                                                                                  \n"," conv2d_52 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_48[0][0]']          \n","                                                                                                  \n"," batch_normalization_47 (BatchN  (None, 16, 16, 32)  128         ['conv2d_52[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_49 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_47[0][0]'] \n","                                                                                                  \n"," conv2d_53 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_49[0][0]']          \n","                                                                                                  \n"," dropout_22 (Dropout)           (None, 16, 16, 32)   0           ['conv2d_53[0][0]']              \n","                                                                                                  \n"," add_22 (Add)                   (None, 16, 16, 32)   0           ['add_21[0][0]',                 \n","                                                                  'dropout_22[0][0]']             \n","                                                                                                  \n"," batch_normalization_48 (BatchN  (None, 16, 16, 32)  128         ['add_22[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_50 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_48[0][0]'] \n","                                                                                                  \n"," conv2d_54 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_50[0][0]']          \n","                                                                                                  \n"," batch_normalization_49 (BatchN  (None, 16, 16, 32)  128         ['conv2d_54[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_51 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_49[0][0]'] \n","                                                                                                  \n"," conv2d_55 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_51[0][0]']          \n","                                                                                                  \n"," dropout_23 (Dropout)           (None, 16, 16, 32)   0           ['conv2d_55[0][0]']              \n","                                                                                                  \n"," add_23 (Add)                   (None, 16, 16, 32)   0           ['add_22[0][0]',                 \n","                                                                  'dropout_23[0][0]']             \n","                                                                                                  \n"," batch_normalization_50 (BatchN  (None, 16, 16, 32)  128         ['add_23[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_52 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_50[0][0]'] \n","                                                                                                  \n"," conv2d_56 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_52[0][0]']          \n","                                                                                                  \n"," batch_normalization_51 (BatchN  (None, 8, 8, 64)    256         ['conv2d_56[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_53 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_51[0][0]'] \n","                                                                                                  \n"," conv2d_57 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_53[0][0]']          \n","                                                                                                  \n"," dropout_24 (Dropout)           (None, 8, 8, 64)     0           ['conv2d_57[0][0]']              \n","                                                                                                  \n"," conv2d_58 (Conv2D)             (None, 8, 8, 64)     2112        ['add_23[0][0]']                 \n","                                                                                                  \n"," add_24 (Add)                   (None, 8, 8, 64)     0           ['dropout_24[0][0]',             \n","                                                                  'conv2d_58[0][0]']              \n","                                                                                                  \n"," batch_normalization_52 (BatchN  (None, 8, 8, 64)    256         ['add_24[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_54 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_52[0][0]'] \n","                                                                                                  \n"," conv2d_59 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_54[0][0]']          \n","                                                                                                  \n"," batch_normalization_53 (BatchN  (None, 8, 8, 64)    256         ['conv2d_59[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_55 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_53[0][0]'] \n","                                                                                                  \n"," conv2d_60 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_55[0][0]']          \n","                                                                                                  \n"," dropout_25 (Dropout)           (None, 8, 8, 64)     0           ['conv2d_60[0][0]']              \n","                                                                                                  \n"," add_25 (Add)                   (None, 8, 8, 64)     0           ['add_24[0][0]',                 \n","                                                                  'dropout_25[0][0]']             \n","                                                                                                  \n"," batch_normalization_54 (BatchN  (None, 8, 8, 64)    256         ['add_25[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_56 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_54[0][0]'] \n","                                                                                                  \n"," conv2d_61 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_56[0][0]']          \n","                                                                                                  \n"," batch_normalization_55 (BatchN  (None, 8, 8, 64)    256         ['conv2d_61[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_57 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_55[0][0]'] \n","                                                                                                  \n"," conv2d_62 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_57[0][0]']          \n","                                                                                                  \n"," dropout_26 (Dropout)           (None, 8, 8, 64)     0           ['conv2d_62[0][0]']              \n","                                                                                                  \n"," add_26 (Add)                   (None, 8, 8, 64)     0           ['add_25[0][0]',                 \n","                                                                  'dropout_26[0][0]']             \n","                                                                                                  \n"," batch_normalization_56 (BatchN  (None, 8, 8, 64)    256         ['add_26[0][0]']                 \n"," ormalization)                                                                                    \n","                                                                                                  \n"," activation_58 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_56[0][0]'] \n","                                                                                                  \n"," global_average_pooling2d_2 (Gl  (None, 64)          0           ['activation_58[0][0]']          \n"," obalAveragePooling2D)                                                                            \n","                                                                                                  \n"," before_softmax (Dense)         (None, 10)           650         ['global_average_pooling2d_2[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," activation_59 (Activation)     (None, 10)           0           ['before_softmax[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 274,442\n","Trainable params: 273,066\n","Non-trainable params: 1,376\n","__________________________________________________________________________________________________\n","x_test.shape (10000, 32, 32, 3)\n"]}],"source":["############################################################## Call_functions  ##########################################\n","\n","data_name=input(\"Enter the dataset             mnist       /        cifar10       / Fashion_mnist /  SVHN  : \")\n","model_name=input(\"Enter the model name    LeNet1 or LeNet5/  12Conv or ResNet20  /     LeNet4    /  LeNet5 : \")\n","x_train, y_train, x_test, y_test, features, model, path= dataset(data_name, model_name)\n","print(data_name, model_name)\n","model.summary()\n","print(\"x_test.shape\", x_test.shape)\n","Clustering_labels, noisy_index , index_withoutnoisy, mis_ind_test, mis_ind_train= FaultFunction(data_name, model_name)"]},{"cell_type":"markdown","metadata":{"id":"I1PMKp5sd93P"},"source":["# **RQ2/RQ3/RQ5**\n","1.Correlation between diversity and Faluts\n","\n","2-Correlation between coverage and faults\n","\n","3-Correlation between diversity and coverage"]},{"cell_type":"markdown","metadata":{"id":"HK--UpLYk2SN"},"source":["## Defining the diversity functions GD (Geometric Diversity), STD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NP1cFVIAR5ST"},"outputs":[],"source":["\n","#________________________________________________(STD function)___________________________________________________________\n","#It takes the feature matrix of the selected subset and compute the STD score and the computation time of the STD\n","\n","#______________Input: Random Subset of test dataset (their extracted features from VGG16)\n","#______________Output: 1- STD score (L1norm)\n","#______________________2- Execution time of STD (timeSTD)\n","\n","def STD(x_sample):\n","  start_time = time.perf_counter()\n","  std_f=np.std(x_sample, axis=0)\n","  L2norm=LA.norm(std_f, 2)\n","  L1norm=LA.norm(std_f, 1)\n","  end_time = time.perf_counter()\n","  print(\"shape x- sample\", x_sample.shape)\n","  print(\"L2norm\", L2norm)\n","  print(\"L1norm\", L1norm)\n","  print('time taken to run:',end_time-start_time)\n","  timeSTD=end_time-start_time\n","  \n","  return L1norm, timeSTD\n","\n","\n","#________________________________________________(GD function)___________________________________________________________\n","\n","# we implemented a function for calculating the GD, you can use it or just run the \"det_mis\" function.__\n","#It takes the feature matrix of the selected subset and compute the GD score and the computation time of the GD\n","#______________Input: Random Subset of test dataset (their extracted features from VGG16)\n","#______________Output: 1- GD score \n","#______________________2- Execution time of GD (timeSTD)\n","\n","# def GD(conv_output):\n","#   start_time = time.perf_counter()\n","#   x_sample = np.zeros((size,len(list(conv_output[0]))))\n","#   conv_output=np.array(conv_output)\n","#   i=0\n","# #input shape\n","#   P = np.zeros((size,32,32,3))\n","#   random_sample.sort()\n","#   for L in random_sample :\n","#     x_sample[i]=conv_output[L]\n","#     P[i]=x_test[L,:,:,:]\n","#     i=i+1\n","#   h=np.dot(x_sample,x_sample.T)\n","#   kko,ldeterminant=np.linalg.slogdet(h)\n","#   end_time = time.perf_counter()\n","#   print(\"lDet2\", abs(ldeterminant))\n","#   print(\"tek\", kko)\n","#   timeGD=end_time-start_time\n","#   return abs(ldeterminant), timeGD\n"]},{"cell_type":"markdown","metadata":{"id":"ipFbGw_blL3A"},"source":["## Defining the coverage functions LSC (Likelihood surprise coverage) and DSC (Distance surprise coverage)"]},{"cell_type":"markdown","source":["### Required setting for White-Box LSC and DSC metrics\n"],"metadata":{"id":"PgKvCF-fu7NA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwUBelnO2Pt_"},"outputs":[],"source":["# You can try other desired layers for each of combinations of models and datasets.\n","\n","model.save(\"/content/drive/MyDrive/sadl11/model/model_Cov.h5\")\n","if data_name == \"cifar10\":\n","  dn=\"cifar\"\n","  if model_name==\"12Conv\":\n","    Pth=\"cifar_test_activation_3_ats.npy\"\n","    layer_names = [\"activation_3\"]\n","  if model_name==\"ResNet20\":\n","    Pth=\"cifar_test_activation_58_ats.npy\"\n","    layer_names=[\"activation_58\"]\n","  np.save(\"/content/drive/MyDrive/sadl11/layer_names.npy\",layer_names)\n","    \n","if data_name == \"Fashion_mnist\":\n","  dn=\"Fashion_mnist\"\n","  Pth=\"Fashion_mnist_test_conv2d_1_ats.npy\"\n","  layer_names = [\"conv2d_1\"]\n","  np.save(\"/content/drive/MyDrive/sadl11/layer_names.npy\",layer_names)\n","\n","if data_name == \"SVHN\":\n","  dn=\"SVHN\"\n","  Pth=\"SVHN_test_conv2d_1_ats.npy\"\n","  layer_names = [\"conv2d_1\"]\n","  np.save(\"/content/drive/MyDrive/sadl11/layer_names.npy\",layer_names)\n","\n","if data_name == \"mnist\":\n","  dn=\"mnist\"\n","  if model_name==\"LeNet1\":\n","    Pth=\"mnist_test_conv2d_1_ats.npy\"\n","    layer_names = [\"conv2d_1\"]\n","  if model_name==\"LeNet5\":\n","    Pth=\"mnist_test_activation_13_ats.npy\"\n","    layer_names = [\"activation_13\"]\n","  np.save(\"/content/drive/MyDrive/sadl11/layer_names.npy\",layer_names)\n"]},{"cell_type":"markdown","source":["### LSC and DSC functions"],"metadata":{"id":"pu6lXNnEvn87"}},{"cell_type":"code","source":["\n","#________________________________________________(DSC and LSC functions)___________________________________________________________\n","#It takes the selected subset and compute the DSC score and the computation time of it\n","\n","#______________Input: Subset of test dataset (images)\n","#______________Output: 1- DSC score (DSC_score) or LSC score (LSC_score)\n","#______________________2- Execution time of DSC and LSC (timeDSC, time LSC)\n","#Note: you can change the upper bound and the number of buckets (LSC's and DSC's hyperparameters)\n","\n","def DSC(input):\n","  np.save(\"/content/drive/MyDrive/sadl11/tmp/x_tcovdsc.npy\",input)\n","  start_time = time.perf_counter()\n","  if data_name == \"cifar10\":\n","    !python run.py -dsa '--d=cifar' '--upper_bound=2'  '--n_bucket=1000'\n","  if data_name == \"mnist\":\n","    !python run.py -dsa '--d=mnist' '--upper_bound=2'  '--n_bucket=1000'\n","  if data_name == \"Fashion_mnist\":\n","    !python run.py -dsa '--d=Fashion_mnist' '--upper_bound=2'  '--n_bucket=1000'\n","  if data_name == \"SVHN\":\n","    !python run.py -dsa '--d=SVHN' '--upper_bound=2'  '--n_bucket=1000'\n","\n","  DSC_score=np.load(\"/content/drive/MyDrive/sadl11/tmp/DSC_cov.npy\")\n","\n","  end_time = time.perf_counter()\n","  print(\"DSC score\",DSC_score)\n","  print('time taken to run:',end_time-start_time)\n","  timeDSC=end_time-start_time\n","  os.remove(\"/content/drive/MyDrive/sadl11/tmp/x_tcovdsc.npy\")\n","  os.remove(\"/content/drive/MyDrive/sadl11/tmp/DSC_cov.npy\")\n","  os.remove('/content/drive/MyDrive/sadl11/tmp/'+ str(Pth))\n","  os.remove('/content/drive/MyDrive/sadl11/tmp/'+ str(dn)+'_test_pred.npy')\n","  return DSC_score,timeDSC\n","\n","def LSC(input):\n","  np.save(\"/content/drive/MyDrive/sadl11/tmp/x_tcovlsc.npy\",input)\n","  start_time = time.perf_counter()\n","  if data_name == \"cifar10\":\n","    !python run.py -lsa '--d=cifar' '--upper_bound=100'  '--n_bucket=1000'\n","  if data_name == \"mnist\":\n","    !python run.py -lsa '--d=mnist'  '--upper_bound=2000'  '--n_bucket=1000'\n","  if data_name == \"Fashion_mnist\":\n","    !python run.py -lsa '--d=Fashion_mnist' '--upper_bound=100'  '--n_bucket=1000'\n","  if data_name == \"SVHN\":\n","    !python run.py -lsa '--d=SVHN' '--upper_bound=100'  '--n_bucket=1000'\n","  \n","  LSC_score=np.load(\"/content/drive/MyDrive/sadl11/tmp/test_cov.npy\")\n","  end_time = time.perf_counter()\n","  print('time taken to run:',end_time-start_time)\n","  timeLSC=end_time-start_time\n","  print(\"####################### LSC_score ###############################\",LSC_score)\n","  os.remove(\"/content/drive/MyDrive/sadl11/tmp/test_cov.npy\")\n","  os.remove(\"/content/drive/MyDrive/sadl11/tmp/x_tcovlsc.npy\")\n","  os.remove('/content/drive/MyDrive/sadl11/tmp/'+str(Pth))\n","  os.remove('/content/drive/MyDrive/sadl11/tmp/'+str(dn)+'_test_pred.npy')\n","  return LSC_score,timeLSC"],"metadata":{"id":"QGrgeZ2evrMy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVA2GnF2_i6e"},"source":["## Defining the coverage functions \n","## KMNC, TKNC, NBC, SNAC, NC\n","Coverage metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhrzZDnV_PgU"},"outputs":[],"source":["#REFERENCE\n","#/***************************************************************************************\n","#*    Title: <Revisiting Neuron Coverage Metrics and Quality of Deep Neural Networks>\n","#*    Author: <\"Zhou Yang\", \"Jieke Shi\", \"Muhammad Hilmi Asyrofi\", \"David Lo\", \"Sina Samangooei\", \"David Dupplaw\">\n","#*    Date: <1/06/2022>\n","#*    Code version: <code version>\n","#*    Availability: <\"https://github.com/soarsmu/Revisiting_Neuron_Coverage/blob/master/Correlation/coverage.py\")>\n","#*\n","#***************************************************************************************/\n","\n","\n","\n","# helper function\n","def get_layer_i_output(model, i, data):\n","    layer_model = K.function([model.layers[0].input], [model.layers[i].output])\n","    ret = layer_model([data])[0]\n","    num = data.shape[0]\n","    ret = np.reshape(ret, (num, -1))\n","    return ret\n","\n","\n","class Coverage:\n","    def __init__(self, model, x_train, y_train, x_test, y_test, x_adv):\n","        self.model = model\n","        self.x_train = x_train\n","        self.y_train = y_train\n","        self.x_test = x_test\n","        self.y_test = y_test\n","        self.x_adv = x_adv\n","\n","    # find scale factors and min num\n","    def scale(self, layers, batch=1024):\n","        data_num = self.x_adv.shape[0]\n","        factors = dict()\n","        for i in layers:\n","            begin, end = 0, batch\n","            max_num, min_num = np.NINF, np.inf\n","            while begin < data_num:\n","                layer_output = get_layer_i_output(self.model, i, self.x_adv[begin:end])\n","                tmp = layer_output.max()\n","                max_num = tmp if tmp > max_num else max_num\n","                tmp = layer_output.min()\n","                min_num = tmp if tmp < min_num else min_num\n","                begin += batch\n","                end += batch\n","            factors[i] = (max_num - min_num, min_num)\n","        return factors\n","\n","    # 1 Neuron Coverage\n","    def NC(self, layers, threshold=0.1, batch=1024):\n","        factors = self.scale(layers, batch=batch)\n","        neuron_num = 0\n","        for i in layers:\n","            out_shape = self.model.layers[i].output.shape\n","            neuron_num += np.prod(out_shape[1:])\n","        neuron_num = int(neuron_num)\n","\n","        activate_num = 0\n","        data_num = self.x_adv.shape[0]\n","        for i in layers:\n","            neurons = np.prod(self.model.layers[i].output.shape[1:])\n","            buckets = np.zeros(neurons).astype('bool')\n","            begin, end = 0, batch\n","            while begin < data_num:\n","                layer_output = get_layer_i_output(self.model, i, self.x_adv[begin:end])\n","                # scale the layer output to (0, 1)\n","                layer_output -= factors[i][1]\n","                layer_output /= factors[i][0]\n","                col_max = np.max(layer_output, axis=0)\n","                begin += batch\n","                end += batch\n","                buckets[col_max > threshold] = True\n","            activate_num += np.sum(buckets)\n","        # print('NC:\\t{:.3f} activate_num:\\t{} neuron_num:\\t{}'.format(activate_num / neuron_num, activate_num, neuron_num))\n","        return activate_num / neuron_num, activate_num, neuron_num\n","\n","    # 2 k-multisection neuron coverage, neuron boundary coverage and strong activation neuron coverage\n","    def KMNC(self, layers, k=1000, batch=1024):\n","        neuron_num = 0\n","        for i in layers:\n","            out_shape = self.model.layers[i].output.shape\n","            neuron_num += np.prod(out_shape[1:])\n","        neuron_num = int(neuron_num)\n","\n","        covered_num = 0\n","        l_covered_num = 0\n","        u_covered_num = 0\n","        for i in layers:\n","            neurons = np.prod(self.model.layers[i].output.shape[1:])\n","            # print(neurons)\n","            begin, end = 0, batch\n","            data_num = self.x_train.shape[0]\n","\n","            neuron_max = np.full(neurons, np.NINF).astype('float')\n","            neuron_min = np.full(neurons, np.inf).astype('float')\n","            while begin < data_num:\n","                layer_output_train = get_layer_i_output(self.model, i, self.x_train[begin:end])\n","                batch_neuron_max = np.max(layer_output_train, axis=0)\n","                batch_neuron_min = np.min(layer_output_train, axis=0)\n","                neuron_max = np.maximum(batch_neuron_max, neuron_max)\n","                neuron_min = np.minimum(batch_neuron_min, neuron_min)\n","                begin += batch\n","                end += batch\n","            buckets = np.zeros((neurons, k + 2)).astype('bool')\n","            interval = (neuron_max - neuron_min) / k\n","            # print(interval[8], neuron_max[8], neuron_min[8])\n","            begin, end = 0, batch\n","            data_num = self.x_adv.shape[0]\n","            while begin < data_num:\n","                layer_output_adv = get_layer_i_output(model, i, self.x_adv[begin: end])\n","                layer_output_adv -= neuron_min\n","                layer_output_adv /= (interval + 10 ** (-100))\n","                layer_output_adv[layer_output_adv < 0.] = -1\n","                layer_output_adv[layer_output_adv >= k / 1.0] = k\n","                layer_output_adv = layer_output_adv.astype('int')\n","                # index 0 for lower, 1 to k for between, k + 1 for upper\n","                layer_output_adv = layer_output_adv + 1\n","                for j in range(neurons):\n","                    uniq = np.unique(layer_output_adv[:, j])\n","                    # print(layer_output_adv[:, j])\n","                    buckets[j, uniq] = True\n","                begin += batch\n","                end += batch\n","            covered_num += np.sum(buckets[:, 1:-1])\n","            u_covered_num += np.sum(buckets[:, -1])\n","            l_covered_num += np.sum(buckets[:, 0])\n","        # print('KMNC:\\t{:.3f} covered_num:\\t{}'.format(covered_num / (neuron_num * k), covered_num))\n","        # print(\n","        #     'NBC:\\t{:.3f} l_covered_num:\\t{}'.format((l_covered_num + u_covered_num) / (neuron_num * 2), l_covered_num))\n","        # print('SNAC:\\t{:.3f} u_covered_num:\\t{}'.format(u_covered_num / neuron_num, u_covered_num))\n","        return covered_num / (neuron_num * k), (l_covered_num + u_covered_num) / (\n","                    neuron_num * 2), u_covered_num / neuron_num, covered_num, l_covered_num, u_covered_num, neuron_num * k\n","\n","    # 4 top-k neuron coverage\n","    def TKNC(self, layers, k=3, batch=1024):\n","        def top_k(x, k):\n","            ind = np.argpartition(x, -k)[-k:]\n","            return ind[np.argsort((-x)[ind])]\n","\n","        neuron_num = 0\n","        for i in layers:\n","            out_shape = self.model.layers[i].output.shape\n","            neuron_num += np.prod(out_shape[1:])\n","        neuron_num = int(neuron_num)\n","\n","        pattern_num = 0\n","        data_num = self.x_adv.shape[0]\n","        for i in layers:\n","            pattern_set = set()\n","            begin, end = 0, batch\n","            while begin < data_num:\n","                layer_output = get_layer_i_output(self.model, i, self.x_adv[begin:end])\n","                topk = np.argpartition(layer_output, -k, axis=1)[:, -k:]\n","                topk = np.sort(topk, axis=1)\n","                # or in order\n","                # topk = np.apply_along_axis[lambda x: top_k(layer_output, k), 1, layer_output]\n","                for j in range(topk.shape[0]):\n","                    pattern_set.add(tuple(topk[j]))\n","                begin += batch\n","                end += batch\n","            pattern_num += len(pattern_set)\n","        # print(\n","        #     'TKNC:\\t{:.3f} pattern_num:\\t{} neuron_num:\\t{}'.format(pattern_num / neuron_num, pattern_num, neuron_num))\n","        return pattern_num / neuron_num, pattern_num, neuron_num\n","\n","    # 4 top-k neuron patterns\n","    def TKNP(self, layers, k=3, batch=1024):\n","        def top_k(x, k):\n","            ind = np.argpartition(x, -k)[-k:]\n","            return ind[np.argsort((-x)[ind])]\n","\n","        def to_tuple(x):\n","            l = list()\n","            for row in x:\n","                l.append(tuple(row))\n","            return tuple(l)\n","\n","        pattern_set = set()\n","        layer_num = len(layers)\n","        data_num = self.x_adv.shape[0]\n","        patterns = np.zeros((data_num, layer_num, k))\n","        layer_cnt = 0\n","        for i in layers:\n","            neurons = np.prod(self.model.layers[i].output.shape[1:])\n","            begin, end = 0, batch\n","            while begin < data_num:\n","                layer_output = get_layer_i_output(self.model, i, self.x_adv[begin:end])\n","                topk = np.argpartition(layer_output, -k, axis=1)[:, -k:]\n","                topk = np.sort(topk, axis=1)\n","                # or in order\n","                # topk = np.apply_along_axis[lambda x: top_k(layer_output, k), 1, layer_output]\n","                patterns[begin:end, layer_cnt, :] = topk\n","                begin += batch\n","                end += batch\n","            layer_cnt += 1\n","\n","        for i in range(patterns.shape[0]):\n","            pattern_set.add(to_tuple(patterns[i]))\n","        pattern_num = len(pattern_set)\n","        print('TKNP:\\t{:.3f}'.format(pattern_num))\n","        return pattern_num\n","\n","    def all(self, layers, batch=100):\n","        self.NC(layers, batch=batch)\n","        self.KMNC(layers, batch=batch)\n","        self.TKNC(layers, batch=batch)\n","        self.TKNP(layers, batch=batch)\n"]},{"cell_type":"markdown","metadata":{"id":"ExL9U2w1KlXm"},"source":["## **Main Function**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHU9569ZL4af"},"outputs":[],"source":["##############################################Function description ############################################################################\n","#________________________________________________(det_mis)___________________________________________________________\n","#It takes the desired size of subset to randomly select it from the test dataset , the corresponding faeture matrix of all inputs in the test dataset,\n","#and the subset (if you have your desired subset, you can give it to the function , it is \"None\") and the model.\n","# It returns all necessary inforamtions for RQ2/RQ3/RQ4/RQ5, the coverage and diversity scores , execution time, and number of faults in each subset.\n","\n","#______________Input: dataset (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\") model_name(\"LeNet1\", \"LeNet5\",\"LeNet4\",\"12Conv\",\"ResNet20\" ), dataset (\"cifar10\",\"cifar100\", \"SVHN\",\"mnist\",\"Fashion_mnist\")\n","#______________Output: 1- Diversity and coverage scores: the GD score \"abs(ldeterminant)\" , the LSC score \"LSCscore\" ,and others:  DSCscore,STDscore, kmnc, nbc, snac, tknc, nc\n","#______________________2- Number of faults : \"faults_n\"\n","#______________________3- Time:  \"timedet\",\"timeLSC\",\"timeDSC\", \"timeSTD\", \"timekmnc\",\"timetknc\",\"timenc\"\n","\n","\n","\n","#remove index of noisy inputs\n","id_test=index_withoutnoisy\n","def det_mis(size , conv_output, subset, model):  \n","\n","    Det=0.0\n","    kko=0.0\n","    tek=0\n","    import time\n","    if subset==None:\n","      while (kko==0.0):\n","        random_sample=random.sample(id_test, size)\n","        random_sample=list(random_sample)\n","        print( len(random_sample))\n","        x_sample = np.zeros((size,len(list(conv_output[0]))))\n","        conv_output=np.array(conv_output)\n","        i=0\n","      #input shape\n","        P = np.zeros_like(x_test[:size])\n","        print(\"PPPPPPPPP\",P.shape)\n","        random_sample.sort()\n","        for L in random_sample :\n","          x_sample[i]=conv_output[L]\n","          P[i]=x_test[L,:,:,:]\n","          i=i+1\n","        t1 = time.perf_counter()\n","        h=np.dot(x_sample,x_sample.T)\n","        kko,ldeterminant=np.linalg.slogdet(h)\n","        t2 = time.perf_counter()\n","        print('time taken to run GD:',t2-t1)\n","        timedet=t2-t1\n","        # print(\"Det1\", np.linalg.det(h))\n","        print(\"lDet2\", abs(ldeterminant))\n","        print(\"tek\", kko)\n","        tek=tek+1\n","    else:\n","      random_sample=list(subset)\n","      x_sample = np.zeros((size,len(list(conv_output[0]))))\n","      conv_output=np.array(conv_output)\n","      i=0\n","    #input shape\n","      P = np.zeros_like(x_test[:size])\n","      print(\"PPPPPPPPP\",P.shape)\n","      random_sample.sort()\n","      for L in random_sample :\n","        x_sample[i]=conv_output[L]\n","        P[i]=x_test[L,:,:,:]\n","        i=i+1\n","      t1 = time.perf_counter()\n","      ### code goes here ###\n","      h=np.dot(x_sample,x_sample.T)\n","      kko,ldeterminant=np.linalg.slogdet(h)\n","      t2 = time.perf_counter()\n","      print('time taken to run:',t2-t1)\n","      timedet=t2-t1\n","      # print(\"Det1\", np.linalg.det(h))\n","      print(\"lDet2\", abs(ldeterminant))\n","    print(\"P shape\", P.shape)\n","    #WB coverage metrics\n","    LSCscore , timeLSC =LSC(P)\n","    DSCscore , timeDSC =DSC(P)\n","    t1 = time.perf_counter()\n","    coverage = Coverage(model, x_train, y_train, x_test, y_test, P)\n","    model_layer = len(model.layers)\n","    lay = range(model_layer)\n","    t2=time.perf_counter()\n","    kmnc, nbc, snac,_ , _,_,_= coverage.KMNC(lay)\n","    t3 = time.perf_counter()\n","    print('time taken to run:',t3-t1)\n","    timekmnc=t3-t1\n","    t4 = time.perf_counter()\n","    tknc,_ , _=coverage.TKNC(lay)\n","    t5 = time.perf_counter()\n","    nc,_ , _=coverage.NC(lay)\n","    t6 = time.perf_counter()\n","    timenc=((t6-t5)+(t2-t1))\n","    print('KMNC: {} NBC: {} SNAC: {} TKNC: {} NC: {}'.format(kmnc, nbc, snac, tknc, nc))\n","    timetknc=((t5-t4)+(t2-t1))\n","\n","    #BB diversity method \n","    STDscore, timeSTD= STD(x_sample)\n","    ddd=model(P)\n","    p_inverted = np.argmax(ddd , axis=1)\n","    pos=0\n","    neg=0\n","    i=0\n","    cluster_lab=[]\n","    nn=0\n","    for l in random_sample:\n","      \n","      if (p_inverted[i]==y_test[l]):\n","        pos=pos+1\n","      else:\n","        neg=neg+1 \n","        # print(\"index mis\",l)\n","        ind=list(mis_ind_test).index(l)\n","        if (Clustering_labels[ind]>-1):\n","          cluster_lab.append(Clustering_labels[ind])\n","        if (Clustering_labels[ind]==-1):\n","          cluster_lab.append(ind)\n","          nn=nn+1\n","      i=i+1 \n","    faults_n=len(list(set(cluster_lab)))\n","    noisy=list(Clustering_labels).count(-1)\n","    allwithnoise=len(list(set(Clustering_labels)))\n","    mispridiction_rate=faults_n/(noisy+allwithnoise-1)\n","    \n","    return abs(ldeterminant), LSCscore ,faults_n, DSCscore,STDscore,timedet,timeLSC,timeDSC, timeSTD, kmnc, nbc, snac, tknc, nc,timekmnc,timetknc,timenc\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-Kh8uv9Minp"},"outputs":[],"source":["#####################################################################Call functions##################################################\n","\n","number_r=0\n","Size0=100\n","Size1=200\n","Size2=300\n","Size3=400\n","Size4=1000\n","\n","X_scaled=features\n","##################################### The number of test subsets that we want from each size: N_repeat\n","N_Repeat=60\n","\n","Cov0=[]\n","mis0=[]\n","R0=[]\n","ld0=[]\n","tdet0=[]\n","tlsc0=[]\n","tdsc0=[]\n","STD0=[]\n","Tstd0=[]\n","kmnc0=[] \n","nbc0=[] \n","snac0=[] \n","tknc0=[]\n","nc0=[]\n","timekmnc0=[]\n","timetknc0=[]\n","timenc0=[]\n","number_r=0\n","\n","size=Size0\n","for i in range(N_Repeat):\n","#If you don't have any saved subset you can write None for the subset argument\n","  subset=np.load(path+\"100_\"+ str(number_r)+\".npy\")\n","  subset=list(subset)\n","  Ld0, cov0 , m0, r0, std0,tdet00,tlsc00,tdsc00,tstd0, kmnc, nbc, snac, tknc, nc, timekmnc, timetknc, timenc =det_mis(Size0, X_scaled, subset, model)\n","  \n","  timekmnc0.append(timekmnc)\n","  timetknc0.append(timetknc)\n","  timenc0.append(timenc)\n","  tdet0.append(tdet00)\n","  tlsc0.append(tlsc00)\n","  tdsc0.append(tdsc00)\n","  Cov0.append(cov0)\n","  mis0.append(m0)\n","  ld0.append(Ld0)\n","  R0.append(r0)\n","  STD0.append(std0)\n","  Tstd0.append(tstd0)\n","  kmnc0.append(kmnc)\n","  nbc0.append(nbc)\n","  snac0.append(snac)\n","  tknc0.append(tknc)\n","  nc0.append(nc)\n","  number_r=number_r+1\n","\n","number_r=0 \n","timekmnc1=[]\n","timetknc1=[]\n","timenc1=[]\n","kmnc1=[] \n","nbc1=[] \n","snac1=[] \n","tknc1=[]\n","nc1=[]\n","mis1=[]\n","Cov1=[]\n","R1=[]\n","ld1=[]\n","tdet1=[]\n","tlsc1=[]\n","tdsc1=[]\n","STD1=[]\n","Tstd1=[]   \n","Size1=200\n","size=Size1\n","for i in range(N_Repeat):\n","  #If you don't have any saved subset you can write None for the subset argument\n","  subset=np.load(path+\"200_\"+ str(number_r)+\".npy\")\n","  subset=list(subset)\n","  Ld1,cov1 , m1,r1,std1,tdet,tlsc,tdsc,tstd1, kmnc, nbc, snac, tknc, nc, timekmnc,timetknc,timenc=det_mis(Size1,X_scaled, subset, model)\n","  timekmnc1.append(timekmnc)\n","  timetknc1.append(timetknc)\n","  timenc1.append(timenc)\n","  tdet1.append(tdet)\n","  tlsc1.append(tlsc)\n","  tdsc1.append(tdsc)\n","  Cov1.append(cov1)\n","  mis1.append(m1)\n","  ld1.append(Ld1)\n","  R1.append(r1)\n","  STD1.append(std1)\n","  Tstd1.append(tstd1)\n","  kmnc1.append(kmnc)\n","  nbc1.append(nbc)\n","  snac1.append(snac)\n","  tknc1.append(tknc)\n","  nc1.append(nc)\n","  number_r=number_r+1\n","\n","\n","\n","ld2=[]\n","R2=[]\n","Cov2=[]\n","mis2=[]\n","tdet2=[]\n","tlsc2=[]\n","tdsc2=[]\n","STD2=[]\n","Tstd2=[]\n","kmnc2=[] \n","nbc2=[] \n","snac2=[] \n","tknc2=[]\n","nc2=[]\n","timekmnc2=[]\n","timetknc2=[]\n","timenc2=[]\n","\n","number_r=0\n","for i in range(N_Repeat):\n","  size=Size2\n","  subset=np.load(path+\"300_\"+ str(number_r)+\".npy\")\n","  subset=list(subset)\n","  Ld2,cov2 , m2,r2,std2,tdet,tlsc,tdsc,tstd2, kmnc, nbc, snac, tknc, nc, timekmnc,timetknc, timenc=det_mis(Size2,X_scaled,subset, model)\n","  timekmnc2.append(timekmnc)\n","  timetknc2.append(timetknc)\n","  timenc2.append(timenc)\n","  tdet2.append(tdet)\n","  tlsc2.append(tlsc)\n","  tdsc2.append(tdsc)\n","  Cov2.append(cov2)\n","  mis2.append(m2)\n","  ld2.append(Ld2)\n","  R2.append(r2)\n","  STD2.append(std2)\n","  Tstd2.append(tstd2)\n","  kmnc2.append(kmnc)\n","  nbc2.append(nbc)\n","  snac2.append(snac)\n","  tknc2.append(tknc)\n","  nc2.append(nc)\n","  number_r=number_r+1\n","\n","number_r=0\n","mis3=[]\n","Cov3=[]\n","ld3=[]\n","R3=[]\n","tdet3=[]\n","tlsc3=[]\n","tdsc3=[]\n","STD3=[]\n","Tstd3=[]\n","kmnc3=[] \n","nbc3=[] \n","snac3=[] \n","tknc3=[]\n","nc3=[]\n","timekmnc3=[]\n","timetknc3=[]\n","timenc3=[]\n","number_r=0\n","for i in range(N_Repeat):\n","  size=Size3\n","  subset=np.load(path+\"400_\"+ str(number_r)+\".npy\")\n","  subset=list(subset)\n","  Ld3,cov3 , m3,r3,std3,tdet,tlsc,tdsc,tstd3, kmnc, nbc, snac, tknc, nc, timekmnc, timetknc, timenc=det_mis(Size3,X_scaled , subset, model)\n","  timekmnc3.append(timekmnc)\n","  timetknc3.append(timetknc)\n","  timenc3.append(timenc)\n","  tdet3.append(tdet)\n","  tlsc3.append(tlsc)\n","  tdsc3.append(tdsc)\n","  Cov3.append(cov3)\n","  mis3.append(m3)\n","  ld3.append(Ld3)\n","  R3.append(r3)\n","  STD3.append(std3)\n","  Tstd3.append(tstd3)\n","  kmnc3.append(kmnc)\n","  nbc3.append(nbc)\n","  snac3.append(snac)\n","  tknc3.append(tknc)\n","  nc3.append(nc)\n","  number_r=number_r+1\n","\n","\n","N_Repeat=60\n","number_r=0 \n","timekmnc4=[]\n","timetknc4=[]\n","timenc4=[]\n","mis4=[]\n","Cov4=[]\n","R4=[]\n","ld4=[]\n","tdet4=[]\n","tlsc4=[]\n","tdsc4=[]\n","STD4=[]\n","Tstd4=[]\n","kmnc4=[]\n","nbc4=[]\n","snac4=[] \n","tknc4=[]\n","nc4=[]\n","STD4=[]\n","Tstd4=[]\n","size=Size4\n","for i in range(N_Repeat):\n","  subset=np.load(path+\"1000_\"+ str(number_r)+\".npy\")\n","  subset=list(subset)\n","  Ld4,cov4 , m4,r4,std,tdet,tlsc,tdsc,tstd, kmnc, nbc, snac, tknc, nc, timekmnc, timetknc, timenc=det_mis(Size4, X_scaled , subset, model)\n","  timekmnc4.append(timekmnc)\n","  timetknc4.append(timetknc)\n","  timenc4.append(timenc)\n","  tdet4.append(tdet)\n","  tlsc4.append(tlsc)\n","  tdsc4.append(tdsc)\n","  Cov4.append(cov4)\n","  mis4.append(m4)\n","  ld4.append(Ld4)\n","  R4.append(r4)\n","  STD4.append(std)\n","  Tstd4.append(tstd)\n","  kmnc4.append(kmnc)\n","  nbc4.append(nbc)\n","  snac4.append(snac)\n","  tknc4.append(tknc)\n","  nc4.append(nc)\n","  number_r=number_r+1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ThxpwzdC00j"},"outputs":[],"source":["#########################################\n","# NOTE: If you are running your own experiment with different subsets you have to append all of the arrays corresponds to your Gd scores and stored them as one array which is ld\n","# You should do the same for other metrics as well . Then ignore running this cell and run the next cells\n","\n","ld=[]\n","Cov=[]\n","R=[]\n","kmnc=[]\n","nbc=[]\n","mis=[]\n","snac=[]\n","tknc=[]\n","nc=[]\n","std=[]\n","timetknc=[]\n","timenc=[]\n","timekmnc=[]\n","\n","for size in [100,200,300,400,1000]:\n","  ld.append(np.load(path+\"metrics/Ldet_\"+ str(size)+\".npy\"))\n","  std.append(np.load(path+\"metrics/STD_\"+ str(size)+\".npy\"))\n","  Cov.append(np.load(path+\"metrics/LSC_\"+ str(size)+\".npy\"))\n","  R.append(np.load(path+\"metrics/DSC_\"+ str(size)+\".npy\"))\n","  kmnc.append(np.load(path+\"metrics/kmnc_\"+ str(size)+\".npy\"))\n","  nbc.append(np.load(path+\"metrics/nbc_\"+ str(size)+\".npy\"))\n","  snac.append(np.load(path+\"metrics/snac_\"+ str(size)+\".npy\"))\n","  tknc.append(np.load(path+\"metrics/tknc_\"+ str(size)+\".npy\"))\n","  nc.append(np.load(path+\"metrics/nc_\"+ str(size)+\".npy\"))\n","  mis.append(np.load(path+\"metrics/#Faults_\"+ str(size)+\".npy\"))\n","  timetknc.append(np.load(path+\"metrics/timetknc_\"+ str(size)+\".npy\"))\n","  timenc.append(np.load(path+\"metrics/timenc_\"+ str(size)+\".npy\"))\n","  timekmnc.append(np.load(path+\"metrics/timekmnc_\"+ str(size)+\".npy\"))"]},{"cell_type":"markdown","metadata":{"id":"QQAYz8HlORDB"},"source":["## Outlier detection "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlnhVe1v-96X"},"outputs":[],"source":["#In this section we do outlier detection and removing the outliers before calculating the correlation\n","#This setting is for RQ2/RQ3 for RQ5 you need to change the outlier rate\n","\n","O=0\n","pr=np.zeros([5,2])\n","kd=np.zeros([5,2])\n","sp=np.zeros([5,2])\n","pr1=np.zeros([5,2])\n","kd1=np.zeros([5,2])\n","sp1=np.zeros([5,2])\n","pr2=np.zeros([5,2])\n","kd2=np.zeros([5,2])\n","sp2=np.zeros([5,2])\n","\n","kd3=np.zeros([5,2])\n","sp3=np.zeros([5,2])\n","kd4=np.zeros([5,2])\n","sp4=np.zeros([5,2])\n","kd5=np.zeros([5,2])\n","sp5=np.zeros([5,2])\n","kd6=np.zeros([5,2])\n","sp6=np.zeros([5,2])\n","kd7=np.zeros([5,2])\n","sp7=np.zeros([5,2])\n","\n","u=0\n","\n","for k,i,s,j,D,O, km,nb,sn,tk , ncc in zip([ld[0],ld[1],ld[2],ld[3], ld[4]],\n","                     [Cov[0],Cov[1],Cov[2],Cov[3], Cov[4]],\n","                     [std[0],std[1],std[2],std[3], std[4]],\n","                     [mis[0],mis[1],mis[2],mis[3], mis[4]],\n","                     [R[0],R[1],R[2],R[3], R[4]],\n","                     [Size0,Size1,Size2,Size3, Size4], \n","                     [kmnc[0],kmnc[1],kmnc[2],kmnc[3],kmnc[4]], \n","                     [nbc[0],nbc[1],nbc[2],nbc[3],nbc[4]],\n","                     [snac[0],snac[1],snac[2],snac[3],snac[4]],\n","                     [tknc[0],tknc[1],tknc[2],tknc[3],tknc[4]],\n","                     [nc[0],nc[1],nc[2],nc[3],nc[4]]):\n","  sp1[u]=stats.spearmanr(k, j) \n","  print('spearman correlation between (log(det), fault) ',N_Repeat,' subsets with size of',O,\"=\",sp1[u])\n","  kd1[u]=stats.kendalltau(k, j) \n","  print('Kendal correlation between (log(det), fault), ',N_Repeat,' subsets with size of',O,\"=\",kd1[u])\n","  print('spearman correlation between (STD score, fault) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(s, j)) \n","  print('Kendal correlation between (STD score, fault) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(s, j))\n","\n","\n","  sp[u]=stats.spearmanr(i, j) \n","  print('spearman correlation between (LSCcov, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp[u])\n","  kd[u]=stats.kendalltau(i, j) \n","  print('Kendal correlation between (LSCcov, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd[u]) \n","  sp2[u]=stats.spearmanr(D, j) \n","  print('spearman correlation between (DSCcov, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp2[u])\n","  kd2[u]=stats.kendalltau(D, j) \n","  print('Kendal correlation between (DSCcov, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd2[u])\n","\n","  sp3[u]=stats.spearmanr(km, j) \n","  print('spearman correlation between (kmnc, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp3[u])\n","  kd3[u]=stats.kendalltau(km, j) \n","  print('Kendal correlation between (kmnc, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd3[u]) \n","  sp4[u]=stats.spearmanr(nb, j) \n","  print('spearman correlation between (nbc, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp4[u])\n","  kd4[u]=stats.kendalltau(nb, j) \n","  print('Kendal correlation between (nbc, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd4[u])\n","\n","  \n","  sp5[u]=stats.spearmanr(sn, j) \n","  print('spearman correlation between (snac, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp5[u])\n","  kd5[u]=stats.kendalltau(sn, j) \n","  print('Kendal correlation between (snac, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd5[u]) \n","  sp6[u]=stats.spearmanr(tk, j) \n","  print('spearman correlation between (tknc, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp6[u])\n","  kd6[u]=stats.kendalltau(tk, j) \n","  print('Kendal correlation between (tknc, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd6[u])\n","  \n","  sp7[u]=stats.spearmanr(ncc, j) \n","  print('spearman correlation between (nc, fault) ',N_Repeat,' subsets with size of',O,\"=\",sp7[u])\n","  kd7[u]=stats.kendalltau(ncc, j) \n","  print('Kendal correlation between (nc, fault) ',N_Repeat,' subsets with size of',O,\"=\",kd7[u])\n","\n","  u=u+1\n","\n","O=0\n","LB1=[]\n","UB3=[]\n","LBl1=[]\n","UBl3=[]\n","for k,i in zip([ld[0],ld[1],ld[2],ld[3], ld[4]],\n","                     [Cov[0],Cov[1],Cov[2],Cov[3], Cov[4]]):\n","  Q1=np.quantile(i,0.25)\n","  Q3=np.quantile(i,0.75)\n","  IQR=Q3 - Q1\n","  LB1.append(Q1 - (1.5 * IQR))\n","  UB3.append(Q3 + (1.5 * IQR))\n","  Ql1=np.quantile(k,0.25)\n","  Ql3=np.quantile(k,0.75)\n","  IQRl=Ql3 - Ql1\n","  LBl1.append(Ql1 - (1.5 * IQRl))\n","  UBl3.append(Ql3 + (1.5 * IQRl))\n","\n","\n","outliers=[]\n","# lower=2.8559534600458797e+66\n","# upper=0.5e+299\n","p=0\n","myDictld={}\n","myDictcov={}\n","myDictmis={}\n","myDictDSC={}\n","myDictSTD={}\n","\n","myDictkm={}\n","myDictnb={}\n","myDictsn={}\n","myDicttk={}\n","myDictncc={}\n","\n","\n","\n","\n","L=0\n","f=0\n","i=0\n","m=0\n","mi=0\n","ooo=[]\n","for L,i,mi, d,s , km, nb, sn,tk,ncc in zip([ld[0],ld[1],ld[2],ld[3], ld[4]],\n","                     [Cov[0],Cov[1],Cov[2],Cov[3], Cov[4]],\n","                      [mis[0],mis[1],mis[2],mis[3], mis[4]],\n","                     [R[0],R[1],R[2],R[3], R[4]],\n","                     [std[0],std[1],std[2],std[3], std[4]],\n","                     [kmnc[0],kmnc[1],kmnc[2],kmnc[3],kmnc[4]], \n","                     [nbc[0],nbc[1],nbc[2],nbc[3],nbc[4]],\n","                     [snac[0],snac[1],snac[2],snac[3],snac[4]],\n","                     [tknc[0],tknc[1],tknc[2],tknc[3],tknc[4]],\n","                     [nc[0],nc[1],nc[2],nc[3],nc[4]]):\n","  \n","  outliers=[]\n","  index=[]\n","  for f in range(len(L)):\n","    lower=LBl1[m]\n","    upper=UBl3[m]\n","    if L[f] < lower or L[f] > upper :\n","      outliers.append(L[f])\n","      ooo.append(L[f])\n","      index.append(f)\n","  print(len(index))\n","  cleanld=np.delete(L,index)\n","  myDictld[str (m)] = list(cleanld)\n","\n","  cleankm=np.delete(km,index)\n","  myDictkm[str (m)] = list(cleankm)\n","\n","  cleannb=np.delete(nb,index)\n","  myDictnb[str (m)] = list(cleannb)\n","  cleansn=np.delete(sn,index)\n","  myDictsn[str (m)] = list(cleansn)\n","  cleantk=np.delete(tk,index)\n","  myDicttk[str (m)] = list(cleantk)\n","  cleanncc=np.delete(ncc,index)\n","  myDictncc[str (m)] = list(cleanncc)\n","\n","  cleancov=np.delete(i,index)\n","  myDictcov[str (m)] = list(cleancov)\n","  cleanDSCcov=np.delete(d,index)\n","  myDictDSC[str (m)] = list(cleanDSCcov)\n","  cleanSTD=np.delete(s,index)\n","  myDictSTD[str (m)] = list(cleanSTD)\n","  cleanmis=np.delete(mi,index)\n","  myDictmis[str (m)] = list(cleanmis)\n","  m=m+1\n","  print('Len of Identified outliers of 100 subsets based on determinant values: %d' % len(outliers), \"m=\",m)\n","print('Len of Identified outliers of all subsets of all sizes: %d' % len(ooo))\n","\n","listcov=list(myDictcov['0'])\n","listcov.extend(list(myDictcov['1']))\n","listcov.extend(list(myDictcov['2']))\n","listcov.extend(list(myDictcov['3']))\n","listcov.extend(list(myDictcov['4']))\n","\n","\n","listmis=list(myDictmis['0'])\n","listmis.extend(list(myDictmis['1']))\n","listmis.extend(list(myDictmis['2']))\n","listmis.extend(list(myDictmis['3']))\n","listmis.extend(list(myDictmis['4']))\n","\n","listld=list(myDictld['0'])\n","listld.extend(list(myDictld['1']))\n","listld.extend(list(myDictld['2']))\n","listld.extend(list(myDictld['3']))\n","listld.extend(list(myDictld['4']))\n","\n","\n","listDSC=list(myDictDSC['0'])\n","listDSC.extend(list(myDictDSC['1']))\n","listDSC.extend(list(myDictDSC['2']))\n","listDSC.extend(list(myDictDSC['3']))\n","listDSC.extend(list(myDictDSC['4']))\n","\n","listSTD=list(myDictSTD['0'])\n","listSTD.extend(list(myDictSTD['1']))\n","listSTD.extend(list(myDictSTD['2']))\n","listSTD.extend(list(myDictSTD['3']))\n","listSTD.extend(list(myDictSTD['4']))\n","\n","listkm=list(myDictkm['0'])\n","listkm.extend(list(myDictkm['1']))\n","listkm.extend(list(myDictkm['2']))\n","listkm.extend(list(myDictkm['3']))\n","listkm.extend(list(myDictkm['4']))\n","\n","listnb=list(myDictnb['0'])\n","listnb.extend(list(myDictnb['1']))\n","listnb.extend(list(myDictnb['2']))\n","listnb.extend(list(myDictnb['3']))\n","listnb.extend(list(myDictnb['4']))\n","\n","listsn=list(myDictsn['0'])\n","listsn.extend(list(myDictsn['1']))\n","listsn.extend(list(myDictsn['2']))\n","listsn.extend(list(myDictsn['3']))\n","listsn.extend(list(myDictsn['4']))\n","\n","\n","listtk=list(myDicttk['0'])\n","listtk.extend(list(myDicttk['1']))\n","listtk.extend(list(myDicttk['2']))\n","listtk.extend(list(myDicttk['3']))\n","listtk.extend(list(myDicttk['4']))\n","\n","\n","listncc=list(myDictncc['0'])\n","listncc.extend(list(myDictncc['1']))\n","listncc.extend(list(myDictncc['2']))\n","listncc.extend(list(myDictncc['3']))\n","listncc.extend(list(myDictncc['4']))\n","\n","subs=np.ones(len(myDictcov['0']))*Size0\n","h=np.ones(len(myDictcov['1']))*Size1\n","hh=np.ones(len(myDictcov['2']))*Size2\n","hhh=np.ones(len(myDictcov['3']))*Size3\n","hhhh=np.ones(len(myDictcov['4']))*Size4\n","subs=list(subs)\n","subs.extend(h)\n","subs.extend(hh)\n","subs.extend(hhh)\n","subs.extend(hhhh)\n","np.array(subs).shape\n","\n","ddd = {'GD scores': listld, 'LSC scores': listcov , 'faults': listmis, 'DSC scores': listDSC, 'STD scores': listSTD, \n","       'kmnc scores': listkm, 'nbc scores': listnb,   'snac scores': listsn, 'tknc scores': listtk,'NC scores': listncc, 'subset':subs}\n","dfcov_log = pd.DataFrame(data=ddd)\n","print(dfcov_log)"]},{"cell_type":"markdown","metadata":{"id":"rkRkjEW8aFRT"},"source":["##Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_zcMdum_f35"},"outputs":[],"source":["#######################################################PLot of correlation results ################################\n","\n","g = sns.scatterplot(x=\"GD scores\", y=\"faults\", hue=\"subset\",\n","              data=dfcov_log, palette='colorblind',  legend='full')\n","g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","plt.show()\n","O=0\n","u=0\n","g = sns.scatterplot(x=\"LSC scores\", y=\"faults\", hue=\"subset\",\n","              data=dfcov_log, palette='colorblind',  legend='full')\n","g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","plt.show()\n","\n","g = sns.scatterplot(x=\"DSC scores\", y=\"faults\", hue=\"subset\",\n","              data=dfcov_log, palette='colorblind',  legend='full')\n","g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","plt.show()\n","for i,j,k,D,S,O, km, tk, sn, nb,ncc in zip([myDictld['0'], myDictld['1'], myDictld['2'], myDictld['3'],myDictld['4']],\n","                     [myDictcov['0'],myDictcov['1'] ,myDictcov['2'] ,myDictcov['3'], myDictcov['4']],\n","                     [myDictmis['0'],myDictmis['1'] ,myDictmis['2'] ,myDictmis['3'], myDictmis['4']],\n","                     [myDictDSC['0'],myDictDSC['1'],myDictDSC['2'],myDictDSC['3'], myDictDSC['4'] ],\n","                     [myDictSTD['0'],myDictSTD['1'],myDictSTD['2'],myDictSTD['3'], myDictSTD['4']],\n","                     [Size0,Size1,Size2,Size3, Size4],\n","                    [myDictkm['0'],myDictkm['1'] ,myDictkm['2'] ,myDictkm['3'], myDictkm['4']],\n","                    [myDicttk['0'],myDicttk['1'] ,myDicttk['2'] ,myDicttk['3'], myDicttk['4']],\n","                    [myDictsn['0'],myDictsn['1'] ,myDictsn['2'] ,myDictsn['3'], myDictsn['4']],\n","                    [myDictnb['0'],myDictnb['1'] ,myDictnb['2'] ,myDictnb['3'], myDictnb['4']],\n","                    [myDictncc['0'],myDictncc['1'] ,myDictncc['2'] ,myDictncc['3'], myDictncc['4']],\n","                    ):\n","  plt.style.use('ggplot')\n","  ii=np.array(i)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'BB GD scores (log of det)': list(i), 'faults': list(k) , 'subset':list(np.ones(len(list(i)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='BB GD scores (log of det)', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (GD, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(i, k))\n","  print('pearson correlation between (GD, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(i, k))\n","  \n","  print('Kendall correlation between (GD, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(i, k))\n","  \n","  plt.pause(0.05)\n","  plt.style.use('ggplot')\n","  jj=np.array(j)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(jj, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'LSC scores': list(j), 'faults': list(k) , 'subset':list(np.ones(len(list(j)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='LSC scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(jj, intercept + slope*jj, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (LSC, mis) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(j, k))\n","  print('pearson correlation between (LSC, mis) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(j, k))\n","  \n","  print('Kendall correlation between (LSC, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(j, k))\n","  \n","\n","  plt.pause(0.05)\n","  plt.style.use('ggplot')\n","  DD=np.array(D)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(DD, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'DSC scores': list(D), 'faults': list(k) , 'subset':list(np.ones(len(list(D)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='DSC scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(DD, intercept + slope*DD, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (DSCcov, Faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(D, k))\n","  print('pearson correlation between (DSCcov, Faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(D, k))\n","  print('Kendall correlation between (DSCcov, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(D, k))\n","  \n","\n","  plt.pause(0.05)\n","  plt.style.use('ggplot')\n","  SS=np.array(S)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(SS, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'STD scores': list(S), 'faults': list(k) , 'subset':list(np.ones(len(list(S)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='STD scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(SS, intercept + slope*SS, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (STD, Faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(S, k))\n","  print('pearson correlation between (STD, Faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(S, k))\n","  print('Kendall correlation between (STD, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(S, k))\n","\n","\n","  plt.style.use('ggplot')\n","  ii=np.array(km)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'kmnc scores': list(km), 'faults': list(k) , 'subset':list(np.ones(len(list(km)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='kmnc scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (kmnc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(km, k))\n","  # print('pearson correlation between (kmnc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(km, k))\n","  \n","  print('Kendall correlation between (kmnc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(km, k))\n","\n","\n","  plt.style.use('ggplot')\n","  ii=np.array(nb)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'nbc scores': list(nb), 'faults': list(k) , 'subset':list(np.ones(len(list(nb)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='nbc scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (nbc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(nb, k))\n","  # print('pearson correlation between (nbc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(nb, k))\n","  \n","  print('Kendall correlation between (nbc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(nb, k))\n"," \n","\n","  plt.style.use('ggplot')\n","  ii=np.array(sn)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'snac scores': list(sn), 'faults': list(k) , 'subset':list(np.ones(len(list(sn)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='snac scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (snac, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(sn, k))\n","  # print('pearson correlation between (snac, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(sn, k))\n","  \n","  print('Kendall correlation between (snac, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(sn, k))\n","\n","  \n","\n","  plt.style.use('ggplot')\n","  ii=np.array(tk)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'tknc scores': list(tk), 'faults': list(k) , 'subset':list(np.ones(len(list(tk)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='tknc scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (tknc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(tk, k))\n","  # print('pearson correlation between (tknc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(tk, k))\n","  print('Kendall correlation between (tknc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(tk, k))\n","\n","  \n","  plt.style.use('ggplot')\n","  ii=np.array(ncc)\n","  kk=np.array(k)\n","  slope, intercept, r, p, stderr = scipy.stats.linregress(ii, kk)\n","  line = f'Regression line: y={intercept:.5f}+{slope:.5f}x'\n","  ddj = {'NC scores': list(ncc), 'faults': list(k) , 'subset':list(np.ones(len(list(ncc)))*O)}\n","  dflo = pd.DataFrame(data=ddj)\n","  fig, g = plt.subplots()\n","  g = sns.scatterplot(x='NC scores', y=\"faults\", hue=\"subset\",\n","                data=dflo, palette='colorblind', \n","                    legend='full')\n","  g.plot(ii, intercept + slope*ii, label=line)\n","  g.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n","  plt.title(str(data_name))\n","  plt.show()\n","  print('spearman correlation between (nc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.spearmanr(ncc, k))\n","  # print('pearson correlation between (nc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.pearsonr(ncc, k))\n","  \n","  print('Kendall correlation between (nc, faults) ',N_Repeat,' subsets with size of',O,\"=\",stats.kendalltau(ncc, k))\n"," \n"," \n"," \n","  u=u+1\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Final Testing Experiment.ipynb","provenance":[{"file_id":"1Tv2fiPVFiMUrvxE9jipWtRDID119xPuI","timestamp":1654006714201},{"file_id":"1UFxDzSHK-B7I0E8hN1O0sveAKaJwV7P2","timestamp":1639150192615}],"mount_file_id":"1CRKye9aSR3sAkAYnvpNrITqnu0VI9R82","authorship_tag":"ABX9TyN6Fo3qeCn4haOKsnduUL1G"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}