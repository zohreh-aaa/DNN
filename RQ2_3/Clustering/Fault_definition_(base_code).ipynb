{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0e9hFRytOqv"
      },
      "source": [
        "## **Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnKN9iRZVnmn",
        "outputId": "3abc96f2-4670-4fac-8aae-d791f560b6f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=6604b5d5f6d66b7b0ec5bbb735790071a68059e4a9d2c3b9d89279c35368cb9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=5601a4d49c81ad6e216d278bd00771c01be7d08fb44b1216d67260749313f847\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.2\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.51.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.29.28)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.21.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tslearn) (3.1.0)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.5.2\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.28.tar.gz (5.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2 MB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.21.5)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.0.2)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (0.29.28)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp37-cp37m-linux_x86_64.whl size=2330832 sha256=cddbb4987207a4328b22ca6717be077783fee0a3135abb35af99c4487e297fbd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/7a/5e/259ccc841c085fc41b99ef4a71e896b62f5161f2bc8a14c97a\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.28\n"
          ]
        }
      ],
      "source": [
        "!pip install umap-learn\n",
        "\n",
        "!pip install tslearn\n",
        "!pip install hdbscan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUcctoxgpJkm"
      },
      "source": [
        "### 1- Test data clustering (from scratch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEgusBIWYkWW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import hdbscan\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Plotting keywords\n",
        "plot_kwds = {'alpha': 0.15, 's': 80, 'linewidths': 0}\n",
        "\n",
        "def plot_clusters(data, algorithm, args, kwds):\n",
        "    start_time = time.time()\n",
        "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
        "\n",
        "    print(\"Labels:\", labels)\n",
        "    print(\"Max label:\", labels.max())\n",
        "\n",
        "    label_copy = copy.deepcopy(labels)\n",
        "    label_list = list(label_copy)\n",
        "    sorted_labels = sorted(label_copy)\n",
        "\n",
        "    for i in range(-1, sorted_labels.max() + 1):\n",
        "        count = label_list.count(i)\n",
        "        print(f\"Number of inputs in class {i} is {count}\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
        "    colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
        "\n",
        "    plt.scatter(data.T[0], data.T[1], c=colors, **plot_kwds)\n",
        "    frame = plt.gca()\n",
        "    frame.axes.get_xaxis().set_visible(False)\n",
        "    frame.axes.get_yaxis().set_visible(False)\n",
        "    plt.title(f'Clusters found by {algorithm.__name__}', fontsize=24)\n",
        "    plt.text(-0.5, 0.7, f'Clustering took {end_time - start_time:.2f} s', fontsize=14)\n",
        "\n",
        "def scale_one(X):\n",
        "    return (X - X.min()) / (X.max() - X.min())\n",
        "\n",
        "# Example usage model is DNN, x_test is cifar10 test dataset (10000,32,32,3), check the necessary normalization before feeding to model usually (0-1) in our paper and models we have used.\n",
        "\n",
        "Y_pred = model(x_test)\n",
        "Y_pred = np.argmax(Y_pred, axis=1)\n",
        "print(Y_pred)\n",
        "YP_Scaled = scale_one(Y_pred)\n",
        "YT_Scaled = scale_one(y_test)\n",
        "\n",
        "# Features and scaled labels: \"features\" are VGG16 outputs, we have used vgg16 as feature extraction: load vgg16 or its features from our files and continue the process\n",
        "X_features = features\n",
        "TY_scaled = YT_Scaled\n",
        "PY_scaled = YP_Scaled\n",
        "\n",
        "# Add the true and predicted labels to the VGG16 features\n",
        "X_features = np.c_[X_features, TY_scaled, PY_scaled]\n",
        "\n",
        "bb, trace, hdbscan_in_umap, clustering_results = [], [], [], []\n",
        "Sumn = 0\n",
        "# The following values are hyperparameters that you can adjust to find the best clustering results.\n",
        "# Since UMAP and HDBSCAN incorporate randomness in their algorithms, ensure that you save the final settings for your reproducibility. We have saved our clustering results in this repository.\n",
        "\n",
        "for i, j in zip([500, 400, 300, 250], [450, 350, 250, 200]):\n",
        "    for k, o in zip([5, 10, 15, 20, 25], [3, 5, 10, 15, 20]):\n",
        "        for n_n in [0.03, 0.1, 0.25, 0.5]:\n",
        "            fit = umap.UMAP(min_dist=n_n, n_components=i, n_neighbors=k)\n",
        "            u1 = fit.fit_transform(X_features)\n",
        "            fit = umap.UMAP(min_dist=0.1, n_components=j, n_neighbors=o)\n",
        "            u = fit.fit_transform(u1)\n",
        "            u = np.c_[u, TY_scaled, PY_scaled]\n",
        "            print(\"UMAP output shape:\", u.shape)\n",
        "\n",
        "            plot_clusters(u, hdbscan.HDBSCAN, (), {'min_cluster_size': 5})\n",
        "            silhouette_umap = sklearn.metrics.silhouette_score(u, labels)\n",
        "            silhouette_features = sklearn.metrics.silhouette_score(X_features, labels)\n",
        "\n",
        "            print(\"Silhouette Score UMAP:\", silhouette_umap)\n",
        "            print(\"Silhouette Score Features:\", silhouette_features)\n",
        "\n",
        "            if (silhouette_umap >= 0.1 or silhouette_features >= 0.1) and labels.max() + 2 >= 200:\n",
        "                bb.append(labels)\n",
        "                config = [i, j, k, o]\n",
        "                trace.append([i, j, k, o, silhouette_umap, labels.max() + 2, list(labels).count(-1)])\n",
        "                hdbscan_in_umap.append(u)\n",
        "                Sumn += 1\n",
        "\n",
        "                clustering_results.append({\n",
        "                    \"Number of Clusters\": labels.max() + 1,\n",
        "                    \"Silhouette Score\": silhouette_umap,\n",
        "                    \"Number of Noisy Inputs\": list(labels).count(-1),\n",
        "                    \"Config\": config\n",
        "                })\n",
        "\n",
        "                print(f\"Iteration {Sumn}: Noisy labels count: {list(labels).count(-1)}\")\n",
        "\n",
        "# Save the results example:\n",
        "# np.save(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/Test_cluster_4068.npy\", bb)\n",
        "# np.save(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/all_trace_4068.npy\", trace)\n",
        "# np.save(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/umap_output_config7_4068.npy\", np.array(hdbscan_in_umap[7]))\n",
        "\n",
        "# Display clustering results in a table and select the one config clustering that has best Silhouette score\n",
        "clustering_df = pd.DataFrame(clustering_results)\n",
        "print(clustering_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMEqJojmpUfA"
      },
      "source": [
        "### 2- Test data clustering (loading the best clustring results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE0Bo0K4pfvc"
      },
      "outputs": [],
      "source": [
        "# Cl_label=np.load(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/Test_cluster_4068.npy\")\n",
        "# trace=np.load(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/all_trace_4068.npy\", allow_pickle=True)\n",
        "# umap_output=(\"/content/drive/MyDrive/RQ_Con_factor/clustering/Cifar10_12Conv/umap_output_4068.npy\")\n"
      ]
    }
  ]
}